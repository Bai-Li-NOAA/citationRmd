@article{Wilson2014,
   author = {Greg Wilson and D. A. Aruliah and C. Titus Brown and Neil P. Chue Hong and Matt Davis and Richard T. Guy and Steven H.D. Haddock and Kathryn D. Huff and Ian M. Mitchell and Mark D. Plumbley and Ben Waugh and Ethan P. White and Paul Wilson},
   doi = {10.1371/journal.pbio.1001745},
   issn = {15457885},
   issue = {1},
   journal = {PLoS Biology},
   pmid = {24415924},
   publisher = {Public Library of Science},
   title = {Best Practices for Scientific Computing},
   volume = {12},
   year = {2014},
}
@article{Wilson2017,
   author = {Greg Wilson and Jennifer Bryan and Karen Cranston and Justin Kitzes and Lex Nederbragt and Tracy K. Teal},
   doi = {10.1371/journal.pcbi.1005510},
   issn = {15537358},
   issue = {6},
   journal = {PLoS Computational Biology},
   month = {6},
   pmid = {28640806},
   publisher = {Public Library of Science},
   title = {Good enough practices in scientific computing},
   volume = {13},
   year = {2017},
}
@article{Li2021,
   abstract = {The National Marine Fisheries Service conducts fishery stock assessments to provide the best scien-tific information available for the U.S. regional fishery management councils. The assessment models applied in the United States are often region specific, although the models share similar math-ematical and statistical attributes. How-ever, comprehensive comparison studies identifying similarities and differences among these assessment models remain scarce. We developed a multi-model comparison framework to evaluate the reliability of 4 age-structured assessment models that are commonly used in the United States: the Assessment Model for Alaska, the Age Structured Assessment Program, the Beaufort Assessment Model, and Stock Synthe-sis. When applied to simulated data, all 4 models produced reliable estimates of assessment quantities of interest, such as fishing mortality, spawning biomass, recruitment, and biological reference points. Although there were differences among models in the calculation of the initial population numbers at age and in the bias adjustment of recruitment, their effects on model outputs were minor when estimation models were configured similarly. In addition, we provide guidelines for converting unfished recruitment and steepness between 2 methods of bias adjustment. We rec-ommend that next-generation stock assessment models include recruitment bias adjustment and that more research be conducted to provide guidelines for which methods might be preferred under which situations.},
   author = {Bai Li and Kyle W. Shertzer and Patrick D. Lynch and James N. Ianelli and Christopher M. Legault and Erik H. Williams and Richard D. Methot and Elizabeth N. Brooks and Jonathan J. Deroba and Aaron M. Berger and Skyler R. Sagarese and Jon K.T. Brodziak and Ian G. Taylor and Melissa A. Karp and Chantel R. Wetzel and Matthew Supernaw},
   doi = {10.7755/FB.119.2-3.5},
   issn = {00900656},
   issue = {2-3},
   journal = {Fishery Bulletin},
   pages = {149-167},
   publisher = {National Marine Fisheries Service},
   title = {A comparison of 4 primary age-structured stock assessment models used in the united states},
   volume = {119},
   year = {2021},
}
@article{White2019,
   abstract = {Most forecasts for the future state of ecological systems are conducted once and never updated or assessed. As a result, many available ecological forecasts are not based on the most up-to-date data, and the scientific progress of ecological forecasting models is slowed by a lack of feedback on how well the forecasts perform. Iterative near-term ecological forecasting involves repeated daily to annual scale forecasts of an ecological system as new data becomes available and regular assessment of the resulting forecasts. We demonstrate how automated iterative near-term forecasting systems for ecology can be constructed by building one to conduct monthly forecasts of rodent abundances at the Portal Project, a long-term study with over 40 years of monthly data. This system automates most aspects of the six stages of converting raw data into new forecasts: data collection, data sharing, data manipulation, modelling and forecasting, archiving, and presentation of the forecasts. The forecasting system uses R code for working with data, fitting models, making forecasts, and archiving and presenting these forecasts. The resulting pipeline is automated using continuous integration (a software development tool) to run the entire pipeline once a week. The cyberinfrastructure is designed for long-term maintainability and to allow the easy addition of new models. Constructing this forecasting system required a team with expertise ranging from field site experience to software development. Automated near-term iterative forecasting systems will allow the science of ecological forecasting to advance more rapidly and provide the most up-to-date forecasts possible for conservation and management. These forecasting systems will also accelerate basic science by allowing new models of natural systems to be quickly implemented and compared to existing models. Using existing technology, and teams with diverse skill sets, it is possible for ecologists to build automated forecasting systems and use them to advance our understanding of natural systems.},
   author = {Ethan P. White and Glenda M. Yenni and Shawn D. Taylor and Erica M. Christensen and Ellen K. Bledsoe and Juniper L. Simonis and S. K.Morgan Ernest},
   doi = {10.1111/2041-210X.13104},
   issn = {2041210X},
   issue = {3},
   journal = {Methods in Ecology and Evolution},
   keywords = {Portal Project,forecasting,iterative forecasting,mammals,prediction},
   month = {3},
   pages = {332-344},
   publisher = {British Ecological Society},
   title = {Developing an automated iterative near-term forecasting system for an ecological study},
   volume = {10},
   year = {2019},
}
@generic{Joppa2013,
   abstract = {"Blind trust" is dangerous when choosing software to support research.},
   author = {Lucas N. Joppa and Greg McInerny and Richard Harper and Lara Salido and Kenji Takeda and Kenton O'Hara and David Gavaghan and Stephen Emmott},
   doi = {10.1126/science.1231535},
   issn = {10959203},
   issue = {6134},
   journal = {Science},
   month = {5},
   pages = {814-815},
   pmid = {23687031},
   publisher = {American Association for the Advancement of Science},
   title = {Troubling trends in scientific software use},
   volume = {340},
   year = {2013},
}
@generic{Dichmont2016,
   abstract = {Stock assessments provide scientific advice in support of fisheries decision making. Ideally, assessments involve fitting population dynamics models to fishery and monitoring data to provide estimates of time-trajectories of biomass and fishing mortality in absolute terms and relative to biological reference points such as BMSY and FMSY, along with measures of uncertainty. Some stock assessments are conducted using software developed for a specific stock or group of stocks. However, increasingly, stock assessments are being conducted using packages developed for application to several taxa and across multiple regions. We review the range of packages used to conduct assessments of fish and invertebrate stocks in the United States because these assessments tend to have common goals, and need to provide similar outputs for decision making. Sixteen packages are considered, five based on surplus production models, one based on a delay-difference model, and the remainder based on age-structured models. Most of the packages are freely available for use by analysts in the US and around the world, have been evaluated using simulations, and can form the basis for forecasts. The packages differ in their ease of use and the types of data inputs they can use. This paper highlights the benefits of stock assessment packages in terms of allowing analysts to explore many assessment configurations and facilitating the peer-review of assessments. It also highlights the disadvantages associated with the use of packages for conducting assessments. Packages with the most options and greatest flexibility are the most difficult to use, and see the greatest development of auxiliary tools to facilitate their use.},
   author = {Catherine M. Dichmont and Roy A. Deng and Andre E. Punt and Jon Brodziak and Yi Jay Chang and Jason M. Cope and James N. Ianelli and Christopher M. Legault and Richard D. Methot and Clay E. Porch and Michael H. Prager and Kyle W. Shertzer},
   doi = {10.1016/j.fishres.2016.07.001},
   issn = {01657836},
   journal = {Fisheries Research},
   keywords = {Fishing mortality,Population dynamics,Reference points,Stock assessment},
   month = {11},
   pages = {447-460},
   publisher = {Elsevier B.V.},
   title = {A review of stock assessment packages in the United States},
   volume = {183},
   year = {2016},
}
@article{Jakeman2006,
   abstract = {Models are increasingly being relied upon to inform and support natural resource management. They are incorporating an ever broader range of disciplines and now often confront people without strong quantitative or model-building backgrounds. These trends imply a need for wider awareness of what constitutes good model-development practice, including reporting of models to users and sceptical review of models by users. To this end the paper outlines ten basic steps of good, disciplined model practice. The aim is to develop purposeful, credible models from data and prior knowledge, in consort with end-users, with every stage open to critical review and revision. Best practice entails identifying clearly the clients and objectives of the modelling exercise; documenting the nature (quantity, quality, limitations) of the data used to construct and test the model; providing a strong rationale for the choice of model family and features (encompassing review of alternative approaches); justifying the techniques used to calibrate the model; serious analysis, testing and discussion of model performance; and making a resultant statement of model assumptions, utility, accuracy, limitations, and scope for improvement. In natural resource management applications, these steps will be a learning process, even a partnership, between model developers, clients and other interested parties. © 2006 Elsevier Ltd. All rights reserved.},
   author = {A. J. Jakeman and R. A. Letcher and J. P. Norton},
   doi = {10.1016/j.envsoft.2006.01.004},
   issn = {13648152},
   issue = {5},
   journal = {Environmental Modelling and Software},
   keywords = {Integrated assessment,Model testing,Sensitivity,System identification,Uncertainty,Verification},
   month = {5},
   pages = {602-614},
   title = {Ten iterative steps in development and evaluation of environmental models},
   volume = {21},
   year = {2006},
}
@generic{Hunter-Zinck2021,
   abstract = {Functional, usable, and maintainable open-source software is increasingly essential to scientific research, but there is a large variation in formal training for software development and maintainability. Here, we propose 10 "rules"centered on 2 best practice components: clean code and testing. These 2 areas are relatively straightforward and provide substantial utility relative to the learning investment. Adopting clean code practices helps to standardize and organize software code in order to enhance readability and reduce cognitive load for both the initial developer and subsequent contributors; this allows developers to concentrate on core functionality and reduce errors. Clean coding styles make software code more amenable to testing, including unit tests that work best with modular and consistent software code. Unit tests interrogate specific and isolated coding behavior to reduce coding errors and ensure intended functionality, especially as code increases in complexity; unit tests also implicitly provide example usages of code. Other forms of testing are geared to discover erroneous behavior arising from unexpected inputs or emerging from the interaction of complex codebases. Although conforming to coding styles and designing tests can add time to the software development project in the short term, these foundational tools can help to improve the correctness, quality, usability, and maintainability of open-source scientific software code. They also advance the principal point of scientific research: producing accurate results in a reproducible way. In addition to suggesting several tips for getting started with clean code and testing practices, we recommend numerous tools for the popular opensource scientific software languages Python, R, and Julia.},
   author = {Haley Hunter-Zinck and Alexandre Fioravante De Siqueira and Váleri N. Vásquez and Richard Barnes and Ciera C. Martinez},
   doi = {10.1371/journal.pcbi.1009481},
   issn = {15537358},
   issue = {11},
   journal = {PLoS Computational Biology},
   month = {11},
   pmid = {34762641},
   publisher = {Public Library of Science},
   title = {Ten simple rules on writing clean and reliable open-source scientific software},
   volume = {17},
   year = {2021},
}
